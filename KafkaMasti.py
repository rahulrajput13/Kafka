## In this simple project I will be attempting to build a simple project to understand Kafka
## This will involve creating a fake stream of messages using the faker library
## I will be attempting to explain each step as I build out this mini-project

## Starting with understanding how the architecture of Kafka works:

## "Apache Kafka is an event streaming platform used to collect, process, store, and integrate data at 
## scale. It has numerous use cases including distributed logging, stream processing, data integration, 
## and pub/sub messaging."

## Event - "An event is any type of action, incident, or change that's identified or recorded by software or applications. 
## For example, a payment, a website click, or a temperature reading, along with a description of what happened."

## Log - An event being stored is called a log.

## Commit Log - A commit log (also known as a write-ahead log or transaction log) is a persistent, ordered, 
## append-only data structure that records the history of changes or events in a system, typically used in 
## databases and distributed systems. The primary purpose of a commit log is to ensure data durability, 
## consistency, and recoverability in the face of system crashes, hardware failures, or software errors. A 
## Commit Log can only have records appended to, no other modification can be done.

## Broker - A Machine on which the data is stored. Kafka servers that store, process, and transmit the messages
## Cluster - Multiple brokers form a Cluster. When a producer sends a message to a topic, it is stored on 
## one or more brokers, depending on the replication factor. Having multiple brokers in a cluster provides 
## fault-tolerance and high availability.
## Zookeeper - Zookeeper is a distributed coordination service that Kafka uses to manage and synchronize 
## the configuration data, leader election, and other administrative tasks in the Kafka cluster. 

## Producers - Producers are applications that generate data (also called messages or records) and send it to Kafka. 
## A producer can send data to one or more topics.
## Consumers -  Consumers are applications that read the messages from Kafka. They subscribe to one or 
## more topics and process the data as required. Consumers can be organized into consumer groups for load balancing and fault tolerance.

## Producer creates events which are sent to a Kafka Cluster. These events are converted into logs and each log is stored
## in the relevant topic. Each topic is partitioned and stored across multiple brokers - to ensure fault tolerance and 
## scalability. The consumers then actively request for logs. The logs are sent from the relevant topic. The consumer can keep
## track of their position within a partition by using offset.

## Methods for partitioning - Round-Robin, Key-Value, Custom.



# Importing Relevant Libraries
from confluent_kafka import Producer, Consumer, KafkaException
import json
from faker import Faker

fake = Faker()

# Creating Producer
p = Producer({'bootstrap.servers': 'localhost:9092'})

# Defining function for callback - takes 2 arguments err and msg - these are generated by the produce function
def delivery_report(err, msg):
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

# Generating 10 logs
for _ in range(10):
    user_event = {
        'name': fake.name(),
        'email': fake.email(),
        'event': fake.random_element(elements=('click', 'purchase', 'logout', 'login'))
    }
    # Logs to be sent to 2 topics
    p.produce('topic1', json.dumps(user_event), callback=delivery_report)
    p.produce('topic2', json.dumps(user_event), callback=delivery_report)

p.flush()

# The p.flush() method in the context of a Kafka producer is used to ensure that all messages in the producer's 
# internal queue have been sent to the Kafka brokers and that any outstanding delivery reports are processed.

# When you use the produce method to send messages in Kafka, the messages are usually buffered in the producer's 
# internal queue before being sent in batches to the Kafka brokers for better performance and throughput. 
# However, in some situations, you might want to ensure that all the messages in the producer's queue have been 
# sent and all the delivery reports have been processed, for example, when you are terminating your application or performing a graceful shutdown.

# Intialising consumer
c = Consumer({
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'mygroup',
    'auto.offset.reset': 'earliest'
})

# Assigning consumer to fetch logs from the topics the producer sent messages to
c.subscribe(['topic1', 'topic2'])

while True:
    msg = c.poll(1.0)

    if msg is None:
        continue
    if msg.error():
        raise KafkaException(msg.error())
    else:
        user_event = json.loads(msg.value().decode('utf-8'))
        print('Received message: {} from topic: {}'.format(user_event, msg.topic()))

c.close()